book_name,difficulty,question,opt_a,opt_b,opt_c,opt_d,answer
textbook_v1,Easy,Which company provided figures 1.9 and 1.15?,IBM,Intel,"Apple Computer, Inc.",Cray Inc.,B
textbook_v1,Easy,Figure 1.11 was courtesy of which corporation?,Storage Technology Corp.,Microsoft Research,AMD,NASA Ames Research Center,A
textbook_v1,Easy,The Charles Babbage Institute provided which figures?,"Figures 1.7.3, 6.13.1, 6.13.3","Figures 1.7.1, 1.7.2, 6.13.2","Figures 7.9.1, 7.9.2","Figures 9.11.2, 9.11.3",B
textbook_v1,Easy,"Which company's figures are 1.7.3, 6.13.1, and 6.13.3?","Apple Computer, Inc.",Cray Inc.,IBM,Intel,C
textbook_v1,Easy,Figure 1.7.4 was courtesy of which company?,AMD,Cray Inc.,Storage Technology Corp.,IBM,B
textbook_v1,Easy,Who provided Figure 1.7.5?,The Computer History Museum,"Apple Computer, Inc.",Charles Babbage Institute,Intel,B
textbook_v1,Easy,Figure 1.7.6 was courtesy of which institution?,The Computer Museum of America,The Commercial Computing Museum,The Computer History Museum,Brown University,C
textbook_v1,Easy,Figure 7.33 was courtesy of which company?,AMD,Intel,IBM,"Apple Computer, Inc.",A
textbook_v1,Easy,Which museum provided figures 7.9.1 and 7.9.2?,The Computer History Museum,"Museum of Science, Boston",The Computer Museum of America,Commercial Computing Museum,B
textbook_v1,Easy,Figure 7.9.4 was courtesy of which company?,"MIPS Technologies, Inc.",AMD,IBM,Intel,A
textbook_v1,Medium,"In the MIPS code for clearing an array using indices, what is the purpose of the instruction `sll $t1,$t0,2`?",It increments the loop counter `i` by 1.,It calculates the memory address of `array[i]` by multiplying `i` by 4.,It checks if the loop counter `i` is less than the array size.,It stores the value 0 into the current array element.,B
textbook_v1,Medium,"In the pointer version of the MIPS code for clearing an array, what does the instruction `addi $t0,$t0,4` achieve?",It sets the pointer `p` to the address of the first element of the array.,It stores the value 0 into the memory location pointed to by `p`.,It increments the pointer `p` to point to the next integer (4 bytes away).,It calculates the address of the last element of the array.,C
textbook_v1,Medium,The text mentions that the pointer version of the array clearing code reduces the number of instructions executed per iteration from 7 to 4. This optimization is analogous to which compiler optimization technique?,Loop unrolling,Strength reduction and induction variable elimination,Function inlining,Constant folding,B
textbook_v1,Medium,"When comparing the index-based and pointer-based MIPS code for clearing an array, the key difference in how addresses are handled within the loop is that the index version:",Uses a fixed address for all array elements.,Recalculates the address from the index in each iteration.,Relies on a separate pointer register that is not updated.,Accesses memory directly without using an address calculation.,B
textbook_v1,Medium,"The text states that the Intel IA-32 architecture evolved over time, with new features added to the original instruction set. Which of the following is NOT mentioned as a significant milestone or addition to the IA-32 architecture?",Introduction of floating-point coprocessor instructions (8087).,Expansion to 32-bit architecture with 32-bit registers (80386).,Addition of a dedicated instruction for matrix multiplication.,Introduction of SIMD instructions like MMX and SSE.,C
textbook_v1,Medium,The IA-32 architecture is described as having 'golden handcuffs' of compatibility. What does this phrase imply about its design evolution?,New features were always backward compatible with older versions.,The need to maintain compatibility with existing software base limited significant architectural changes.,The architecture was designed to be easily compatible with other processor families.,Compatibility was sacrificed for performance in later versions.,B
textbook_v1,Medium,A key difference between MIPS and IA-32 arithmetic and logical instructions is that IA-32:,Requires separate source and destination registers for all operations.,Allows one operand to be in memory for virtually any instruction.,Only supports 32-bit data types.,Uses a stack-based architecture for all operations.,B
textbook_v1,Medium,The 80386 extended the 80286 architecture to 32 bits. What was a consequence of this extension regarding registers?,All 16-bit registers were replaced by new 32-bit registers.,The top eight 16-bit registers were extended to 32 bits and could be used as general-purpose registers.,The number of general-purpose registers was significantly increased to match MIPS.,Segment registers were also extended to 32 bits.,B
textbook_v1,Medium,The text mentions that the Intel 8086 is not considered a general-purpose register architecture because:,It only has a few registers available.,Its registers have dedicated uses.,Most operations require memory operands.,It lacks support for floating-point operations.,B
textbook_v1,Medium,"The AMD64 architecture, introduced by AMD, significantly changed the IA-32 architecture by:",Reducing the number of general-purpose registers to 8.,Increasing the address space to 64 bits and widening all registers to 64 bits.,Eliminating support for 16-bit and 32-bit data types.,Adopting a purely stack-based architecture.,B
textbook_v1,Medium,"According to the text, which two factors are primarily determined by the processor's implementation?",Instruction count and compiler efficiency,Clock cycle time and clock cycles per instruction (CPI),Instruction set architecture and program performance,Compiler optimization and instruction set complexity,B
textbook_v1,Medium,Which of the following is NOT included in the subset of MIPS instructions examined in this chapter?,Load word (lw),Branch equal (beq),Shift instructions,Add (add),C
textbook_v1,Medium,What is the primary purpose of a multiplexor (mux) in the processor implementation described?,To perform arithmetic and logical operations.,To store program instructions.,To select one of several input sources to steer to a destination.,To control the flow of data between registers.,C
textbook_v1,Medium,"In the initial steps of executing any instruction, what are the two identical actions performed?",Read registers and access data memory.,Fetch instruction and read registers.,Calculate address and write to register.,Perform ALU operation and update PC.,B
textbook_v1,Medium,For which instruction class is the ALU used for comparison?,Memory-reference instructions,Arithmetic-logical instructions,Branch instructions,Jump instructions,C
textbook_v1,Medium,What is the main drawback of the simple implementation scheme that uses a single long clock cycle for every instruction?,It requires more complex control logic.,It is slower than implementations that allow different instruction classes to take different numbers of clock cycles.,It cannot handle memory-reference instructions efficiently.,It necessitates the use of microprogramming.,B
textbook_v1,Medium,The text mentions two important aspects omitted in the abstract view of the MIPS implementation (Figure 5.1). One is the need for multiplexors. What is the other?,The need for separate instruction and data caches.,The control of units based on the instruction type.,The use of pipelining for performance.,The implementation of floating-point operations.,B
textbook_v1,Medium,"Which of the following is an example of a 'state element' in digital logic design, as described in the text?",An Arithmetic Logic Unit (ALU),A multiplexor,A register file,A combinational logic circuit,C
textbook_v1,Medium,"According to the text, the simplicity and regularity of the MIPS instruction set simplifies the implementation by:",Allowing for a single clock cycle for all instructions.,Reducing the need for a control unit.,Making the execution of many instruction classes similar.,Eliminating the need for memory access.,C
textbook_v1,Medium,The 'Check Yourself' question suggests that separate instruction and data memories are needed in the single-cycle datapath because:,The MIPS instruction format is fundamentally different from data formats.,Separate memories are a cost-saving measure in single-cycle designs.,The processor's single cycle cannot accommodate two accesses to a single-ported memory.,Instruction fetching and data access require different clock speeds.,C
textbook_v1,Medium,What is the primary purpose of a branch delay slot in a pipelined processor?,To store frequently used branch targets for faster access.,"To hold an instruction that always executes after a branch, aiming to hide branch penalties.",To buffer instructions that are likely to be fetched after a branch.,To detect and correct branch mispredictions.,B
textbook_v1,Medium,"According to the text, why has delayed branching lost popularity in modern processors?",It is too complex to implement in hardware.,It is ineffective for short pipelines.,Longer pipelines and multiple instructions per clock cycle make the branch delay too long for a single slot.,Dynamic prediction schemes are significantly cheaper and more flexible.,C
textbook_v1,Medium,What is a 'correlating predictor' in the context of branch prediction?,A predictor that only uses information about the immediate preceding branch.,A predictor that combines local branch behavior with global behavior of recent branches.,A predictor that uses a cache to store branch targets.,A predictor that always assumes a branch will be taken.,B
textbook_v1,Medium,A 'tournament predictor' differs from a correlating predictor primarily in its:,Use of a single predictor for all branches.,Ability to predict only taken branches.,Inclusion of a selection mechanism to choose among multiple predictors.,Reliance solely on local branch history.,C
textbook_v1,Medium,What is the function of a 'branch target buffer' (BTB)?,To predict whether a branch will be taken or not.,To cache the destination program counter or instruction for a branch.,To store the history of recently executed branches.,To hold instructions that can be placed in the branch delay slot.,B
textbook_v1,Medium,"In the context of exceptions, what is the purpose of the 'EPC' register?",To store the cause of the exception.,To hold the address of the instruction that caused the exception.,To store the next instruction to be fetched after handling the exception.,To buffer instructions that need to be flushed from the pipeline.,B
textbook_v1,Medium,"When an arithmetic overflow exception occurs during the EX stage of a pipelined processor, what is the typical action taken regarding the instruction that caused the overflow?",It is allowed to complete its write-back to the register file.,"It is flushed from the pipeline, and its result is prevented from being written back.",It is immediately restarted from the beginning.,It is replaced by a 'nop' instruction in the EX stage.,B
textbook_v1,Medium,The text suggests that breaking memory accesses into two clock cycles could improve performance in both pipelined and multicycle designs. What is the primary reason for this?,It allows for a faster ALU operation.,It reduces the impact of data hazards.,"It reduces the clock cycle time, which is currently bottlenecked by memory access.",It simplifies the control unit logic.,C
textbook_v1,Medium,Which of the following is NOT listed as a cause of exceptions in the provided text?,Arithmetic overflow,I/O device request,Cache miss,Using an undefined instruction,C
textbook_v1,Medium,"In Figure 6.40(b), the branch delay slot is scheduled from the target of the branch. This strategy is preferred when:",The branch is likely to be not taken.,"The branch is taken with high probability, such as a loop branch.",The instruction at the target is independent of the branch condition.,The instruction at the target uses registers that are not modified by the branch.,B
textbook_v1,Hard,"The text states that a byte address 300 falls into block number 9, which then maps to cache block number 1 (9 modulo 8). What is the fundamental assumption being made about the cache's organization that allows for this modulo operation?",The cache uses a fully associative mapping scheme.,The cache is direct-mapped and the number of cache blocks is a power of 2.,The cache uses a set-associative mapping scheme with a high degree of associativity.,The block offset within the address is used to determine the cache block number.,B
textbook_v1,Hard,"The pitfall regarding ignoring memory system behavior in programming is illustrated with matrix multiplication. If changing the loop order from i,j,k to k,j,i significantly impacts performance, what aspect of memory hierarchy behavior is primarily being exploited or violated?",Write-through versus write-back cache policies.,The principle of temporal locality.,The principle of spatial locality and cache line fill.,The latency of DRAM access.,C
textbook_v1,Hard,"When evaluating the memory hierarchy of an out-of-order processor, the text advises against using average memory access time if the processor continues to execute instructions during a cache miss. What is the core reason this approach is inaccurate in such scenarios?",Out-of-order processors have significantly higher cache miss rates.,Average memory access time doesn't account for the potential for additional misses or instruction execution during a miss.,The processor's execution time is independent of memory hierarchy performance in out-of-order designs.,The memory stall time is always zero in out-of-order processors.,B
textbook_v1,Hard,"The pitfall of extending an address space by adding segments is discussed. The text mentions that this can cause problems when a programming language needs an address larger than one segment, such as for unrestricted pointers. What is the underlying issue that makes this problematic?",Segmented addresses inherently increase the speed of pointer dereferencing.,"The fixed size of segments limits the maximum addressable memory, regardless of the total address bits.",The need to combine segment and offset information for every memory access adds overhead and complexity.,Segmented addressing schemes are incompatible with modern compiler optimizations.,C
textbook_v1,Hard,"Figure 7.36 shows that L1 caches are similar across different microprocessor applications, while L2 cache size and other factors vary. What does this suggest about the primary role of L1 caches versus L2 caches in the memory hierarchy?","L1 caches are primarily for large data sets, while L2 caches handle instruction fetching.","L1 caches are optimized for immediate instruction execution, while L2 caches provide a larger buffer for frequently accessed data.","L1 caches are designed for very low latency access to frequently used instructions and data, while L2 caches offer a larger capacity to reduce misses from L1.","L1 caches are always direct-mapped, whereas L2 caches are typically set-associative.",C
textbook_v1,Hard,"The text states that processor speeds are improving faster than DRAM access times, leading to a growing gap. It also notes that DRAM technology has seen enhancements like double data rate (DDR) DRAMs, leading to greater increases in memory bandwidth. How does increased memory bandwidth, even with slow latency improvement, help mitigate the performance gap?","Higher bandwidth allows for larger cache block sizes, which can reduce the miss penalty.",Higher bandwidth directly reduces the latency of individual memory accesses.,Increased bandwidth enables processors to execute more instructions per clock cycle.,"Higher bandwidth makes it possible to use smaller, faster caches.",A
textbook_v1,Hard,"The text discusses compiler-directed prefetching. If a secondary cache can be involved in a prefetch while the primary cache continues to service processor requests, what is the key advantage of this multi-level approach to prefetching?","It allows the primary cache to be smaller, reducing power consumption.","It prevents the primary cache from experiencing any stalls, even during prefetches.","It decouples the prefetching operation from the immediate needs of the processor, reducing the impact on foreground cache performance.",It guarantees that all prefetched data will be used by the processor.,C
textbook_v1,Hard,"The example of matrix multiplication shows that changing loop order can drastically affect performance due to memory access patterns. If a program exhibits very high spatial locality but very little temporal locality, what kind of memory access pattern would this imply?","Accessing the same memory locations repeatedly, but with large gaps between accesses.","Accessing contiguous memory locations, but each location is accessed only once.","Accessing random memory locations, with no discernible pattern.","Accessing memory locations in a strictly sequential order, but only a few locations are ever revisited.",B
textbook_v1,Hard,"The text mentions that for a second- or third-level cache, it's possible to use larger block sizes because they are not constantly being used by the processor. What is the primary benefit of using larger block sizes in these levels of the hierarchy, assuming it doesn't excessively increase the miss penalty?",It reduces the number of tags required for the cache.,It increases the likelihood of fetching useful data due to spatial locality.,It decreases the overall cache capacity.,It simplifies the cache replacement policy.,B
textbook_v1,Hard,"The pitfall of extending an address space by adding segments is described as problematic because it can turn every address into two words (segment and offset). This implies that operations involving addresses, such as passing them as parameters or storing them in registers, would become less efficient. What is the fundamental reason for this inefficiency?",The processor needs to perform an additional memory lookup for each segment.,The increased size of addresses requires more complex arithmetic operations and potentially more register usage.,Segment registers are slower to access than general-purpose registers.,The compiler cannot optimize away the segment information.,B
